{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib\n",
    "\n",
    "\n",
    "#Transforms\n",
    "transformer=transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                        [0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "\n",
    "#Path for training and testing directory\n",
    "train_path=r\"C:\\Users\\rohin\\Desktop\\New folder (2)\\biomet_pattern_and_people_detection\\Task 2 DeepFakes Detection-20240228\\Task_1\\Task_1\\data_train2\\train\"\n",
    "test_path=r\"C:\\Users\\rohin\\Desktop\\New folder (2)\\biomet_pattern_and_people_detection\\Task 2 DeepFakes Detection-20240228\\Task_1\\Task_1\\data_train2\\test\"\n",
    "\n",
    "root=pathlib.Path(train_path)\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])\n",
    "\n",
    "\n",
    "#CNN Network\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,num_classes=6):\n",
    "        super(ConvNet,self).__init__()\n",
    "        \n",
    "        #Output size after convolution filter\n",
    "        #((w-f+2P)/s) +1\n",
    "        \n",
    "        #Input shape= (256,3,150,150)\n",
    "        \n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (256,12,150,150)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=12)\n",
    "        #Shape= (256,12,150,150)\n",
    "        self.relu1=nn.ReLU()\n",
    "        #Shape= (256,12,150,150)\n",
    "        \n",
    "        self.pool=nn.MaxPool2d(kernel_size=2)\n",
    "        #Reduce the image size be factor 2\n",
    "        #Shape= (256,12,75,75)\n",
    "        \n",
    "        \n",
    "        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (256,20,75,75)\n",
    "        self.relu2=nn.ReLU()\n",
    "        #Shape= (256,20,75,75)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "        #Shape= (256,32,75,75)\n",
    "        self.bn3=nn.BatchNorm2d(num_features=32)\n",
    "        #Shape= (256,32,75,75)\n",
    "        self.relu3=nn.ReLU()\n",
    "        #Shape= (256,32,75,75)\n",
    "        \n",
    "        \n",
    "        self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Feed forwad function\n",
    "        \n",
    "    def forward(self,input):\n",
    "        output=self.conv1(input)\n",
    "        output=self.bn1(output)\n",
    "        output=self.relu1(output)\n",
    "            \n",
    "        output=self.pool(output)\n",
    "            \n",
    "        output=self.conv2(output)\n",
    "        output=self.relu2(output)\n",
    "            \n",
    "        output=self.conv3(output)\n",
    "        output=self.bn3(output)\n",
    "        output=self.relu3(output)\n",
    "            \n",
    "            \n",
    "            #Above output will be in matrix form, with shape (256,32,75,75)\n",
    "            \n",
    "        output=output.view(-1,32*75*75)\n",
    "            \n",
    "            \n",
    "        output=self.fc(output)\n",
    "            \n",
    "        return output\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu1): ReLU()\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(12, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (conv3): Conv2d(20, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu3): ReLU()\n",
      "  (fc): Linear(in_features=180000, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "checkpoint=torch.load('best_checkpoint.model')\n",
    "model=ConvNet(num_classes=2)\n",
    "model.load_state_dict(checkpoint)\n",
    "print(model.eval())    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "#prediction function\n",
    "def prediction(img_path,transformer):\n",
    "    \n",
    "    image=Image.open(img_path)\n",
    "\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    image_tensor=transformer(image).float()\n",
    "    \n",
    "    \n",
    "    image_tensor=image_tensor.unsqueeze_(0)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        image_tensor.cuda()\n",
    "        \n",
    "    input=Variable(image_tensor)\n",
    "    \n",
    "    \n",
    "    output=model(input)\n",
    "    \n",
    "    index=output.data.numpy().argmax()\n",
    "    \n",
    "    pred=classes[index]\n",
    "    \n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'real', 1: 'real', 2: 'real', 3: 'real', 4: 'real', 5: 'real', 6: 'real', 7: 'real', 8: 'real', 9: 'real', 10: 'real', 11: 'real', 12: 'real', 13: 'real', 14: 'fake', 15: 'fake', 16: 'fake', 17: 'real', 18: 'fake', 19: 'fake', 20: 'real', 21: 'real', 22: 'real', 23: 'real', 24: 'real', 25: 'real', 26: 'real', 27: 'real', 28: 'real', 29: 'real', 30: 'real', 31: 'real', 32: 'real', 33: 'real', 34: 'real', 35: 'real', 36: 'real', 37: 'real', 38: 'real', 39: 'real', 40: 'real', 41: 'real', 42: 'real', 43: 'real', 44: 'real', 45: 'real', 46: 'real', 47: 'real', 48: 'real', 49: 'real', 50: 'real', 51: 'real', 52: 'real', 53: 'real', 54: 'real', 55: 'real', 56: 'real', 57: 'real', 58: 'real', 59: 'real', 60: 'real', 61: 'real', 62: 'real', 63: 'real', 64: 'real', 65: 'real', 66: 'real', 67: 'real', 68: 'real', 69: 'real', 70: 'real', 71: 'fake', 72: 'fake', 73: 'fake', 74: 'fake', 75: 'fake', 76: 'fake', 77: 'fake', 78: 'fake', 79: 'fake', 80: 'fake', 81: 'real', 82: 'real', 83: 'real', 84: 'real', 85: 'real', 86: 'real', 87: 'real', 88: 'real', 89: 'real', 90: 'real', 91: 'real', 92: 'real', 93: 'real', 94: 'real', 95: 'real', 96: 'real', 97: 'real', 98: 'real', 99: 'real', 100: 'real', 101: 'real', 102: 'real', 103: 'real', 104: 'real', 105: 'real', 106: 'real', 107: 'real', 108: 'real', 109: 'real', 110: 'real', 111: 'real', 112: 'real', 113: 'real', 114: 'real', 115: 'real', 116: 'real', 117: 'real', 118: 'real', 119: 'real', 120: 'real', 121: 'real', 122: 'real', 123: 'real', 124: 'real', 125: 'real', 126: 'real', 127: 'real', 128: 'real', 129: 'real', 130: 'real', 131: 'real', 132: 'real', 133: 'real', 134: 'real', 135: 'real', 136: 'real', 137: 'real', 138: 'real', 139: 'real', 140: 'real', 141: 'real', 142: 'real', 143: 'real', 144: 'real', 145: 'real', 146: 'real', 147: 'real', 148: 'real', 149: 'real', 150: 'real', 151: 'real', 152: 'real', 153: 'real', 154: 'real', 155: 'real', 156: 'real', 157: 'real', 158: 'real', 159: 'real', 160: 'real', 161: 'real', 162: 'real', 163: 'real', 164: 'real', 165: 'real', 166: 'real', 167: 'real', 168: 'real', 169: 'real', 170: 'real', 171: 'real', 172: 'fake', 173: 'fake', 174: 'fake', 175: 'fake', 176: 'fake', 177: 'fake', 178: 'fake', 179: 'fake', 180: 'real', 181: 'real', 182: 'real', 183: 'real', 184: 'fake', 185: 'fake', 186: 'real', 187: 'real', 188: 'real', 189: 'real', 190: 'fake', 191: 'real', 192: 'real', 193: 'real', 194: 'real', 195: 'real', 196: 'real', 197: 'real', 198: 'real', 199: 'real', 200: 'real', 201: 'real', 202: 'real', 203: 'fake', 204: 'real', 205: 'real', 206: 'real', 207: 'real', 208: 'real', 209: 'real', 210: 'real', 211: 'real', 212: 'real', 213: 'fake', 214: 'real', 215: 'real', 216: 'real', 217: 'real', 218: 'real', 219: 'real', 220: 'real', 221: 'real', 222: 'real', 223: 'real', 224: 'real', 225: 'real', 226: 'real', 227: 'real', 228: 'real', 229: 'real', 230: 'real', 231: 'real', 232: 'real', 233: 'real', 234: 'real', 235: 'real', 236: 'real', 237: 'real', 238: 'real', 239: 'real', 240: 'real', 241: 'real', 242: 'real', 243: 'real', 244: 'fake', 245: 'real', 246: 'real', 247: 'real', 248: 'real', 249: 'real'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "images_path=glob.glob(r\"C:\\Users\\rohin\\Desktop\\New folder (2)\\biomet_pattern_and_people_detection\\Task 2 DeepFakes Detection-20240228\\Task_1\\Task_1\\data_eval\\fake_task2\" +'/*.jpg')\n",
    "\n",
    "pred_dict={}\n",
    "\n",
    "for idx, i in enumerate(images_path):\n",
    "    pred_dict[idx]=prediction(i,transformer)            \n",
    "\n",
    "print(pred_dict)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'real', 1: 'real', 2: 'real', 3: 'real', 4: 'real', 5: 'real', 6: 'real', 7: 'real', 8: 'real', 9: 'real', 10: 'real', 11: 'real', 12: 'real', 13: 'real', 14: 'real', 15: 'real', 16: 'real', 17: 'real', 18: 'fake', 19: 'real', 20: 'real', 21: 'real', 22: 'fake', 23: 'real', 24: 'real', 25: 'real', 26: 'real', 27: 'real', 28: 'real', 29: 'real', 30: 'real', 31: 'real', 32: 'real', 33: 'real', 34: 'real', 35: 'real', 36: 'real', 37: 'real', 38: 'real', 39: 'real', 40: 'real', 41: 'real', 42: 'real', 43: 'real', 44: 'real', 45: 'real', 46: 'real', 47: 'real', 48: 'real', 49: 'real', 50: 'real', 51: 'real', 52: 'real', 53: 'real', 54: 'real', 55: 'real', 56: 'real', 57: 'real', 58: 'real', 59: 'fake', 60: 'real', 61: 'real', 62: 'real', 63: 'real', 64: 'real', 65: 'real', 66: 'real', 67: 'real', 68: 'real', 69: 'real', 70: 'real', 71: 'real', 72: 'real', 73: 'real', 74: 'real', 75: 'real', 76: 'real', 77: 'real', 78: 'fake', 79: 'real', 80: 'fake', 81: 'fake', 82: 'fake', 83: 'real', 84: 'real', 85: 'real', 86: 'real', 87: 'real', 88: 'real', 89: 'real', 90: 'real', 91: 'real', 92: 'real', 93: 'real', 94: 'real', 95: 'real', 96: 'real', 97: 'fake', 98: 'real', 99: 'real', 100: 'real', 101: 'real', 102: 'real', 103: 'real', 104: 'real', 105: 'real', 106: 'real', 107: 'real', 108: 'real', 109: 'real', 110: 'fake', 111: 'real', 112: 'fake', 113: 'fake', 114: 'fake', 115: 'real', 116: 'real', 117: 'real', 118: 'real', 119: 'real', 120: 'real', 121: 'real', 122: 'real', 123: 'real', 124: 'real', 125: 'real', 126: 'real', 127: 'real', 128: 'real', 129: 'real', 130: 'real', 131: 'real', 132: 'real', 133: 'real', 134: 'real', 135: 'real', 136: 'real', 137: 'real', 138: 'real', 139: 'real', 140: 'real', 141: 'real', 142: 'real', 143: 'real', 144: 'real', 145: 'real', 146: 'real', 147: 'real', 148: 'real', 149: 'real', 150: 'real', 151: 'real', 152: 'real', 153: 'real', 154: 'real', 155: 'real', 156: 'real', 157: 'real', 158: 'real', 159: 'real', 160: 'real', 161: 'real', 162: 'real', 163: 'real', 164: 'real', 165: 'real', 166: 'real', 167: 'real', 168: 'real', 169: 'real', 170: 'fake', 171: 'real', 172: 'real', 173: 'real', 174: 'real', 175: 'real', 176: 'real', 177: 'real', 178: 'real', 179: 'real', 180: 'real', 181: 'real', 182: 'fake', 183: 'real', 184: 'real', 185: 'real', 186: 'real', 187: 'real', 188: 'real', 189: 'real', 190: 'real', 191: 'real', 192: 'real', 193: 'real', 194: 'real', 195: 'real', 196: 'real', 197: 'real', 198: 'real', 199: 'real', 200: 'real', 201: 'real', 202: 'real', 203: 'real', 204: 'real', 205: 'real', 206: 'real', 207: 'real', 208: 'real', 209: 'real', 210: 'real', 211: 'real', 212: 'real', 213: 'real', 214: 'real', 215: 'real', 216: 'real', 217: 'real', 218: 'real', 219: 'real', 220: 'real', 221: 'real', 222: 'real', 223: 'real', 224: 'real', 225: 'real', 226: 'real', 227: 'real', 228: 'real', 229: 'real', 230: 'real', 231: 'real', 232: 'real', 233: 'real', 234: 'real', 235: 'real', 236: 'real', 237: 'real', 238: 'real', 239: 'real', 240: 'real', 241: 'real', 242: 'real', 243: 'real', 244: 'real', 245: 'real', 246: 'real', 247: 'real', 248: 'real', 249: 'real', 250: 'real', 251: 'real', 252: 'real', 253: 'fake', 254: 'fake', 255: 'real', 256: 'real', 257: 'real', 258: 'real', 259: 'real', 260: 'real', 261: 'real', 262: 'real'}\n"
     ]
    }
   ],
   "source": [
    "images_path_real=glob.glob(r\"C:\\Users\\rohin\\Desktop\\New folder (2)\\biomet_pattern_and_people_detection\\Task 2 DeepFakes Detection-20240228\\Task_1\\Task_1\\data_eval\\real_task2\" +'/*.jpg')\n",
    "\n",
    "pred_dict_real={}\n",
    "\n",
    "for idx, i in enumerate(images_path_real):\n",
    "    pred_dict_real[idx]=prediction(i,transformer)            \n",
    "\n",
    "print(pred_dict_real)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "true_labels = ['fake'] * len(images_path_real) + ['real'] * len(images_path_real)\n",
    "predicted_labels = list(pred_dict.values()) + list(pred_dict_real.values())\n",
    "\n",
    "correct_predictions = sum(1 for true_label, predicted_label in zip(true_labels, predicted_labels) if true_label == predicted_label)\n",
    "total_predictions = len(true_labels)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ftdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
